
数据爬取：
	爬虫技术： 使用Web爬虫工具（如Scrapy、BeautifulSoup、Selenium等）从互联网上抓取数据。

数据保存：

	数据库： 使用关系型数据库（如MySQL、PostgreSQL）或非关系型数据库（如MongoDB、Redis）存储结构化或非结构化数据。
	文件存储： 将数据保存为文件，如CSV、JSON、XML等格式。
	云存储： 使用云服务（如Amazon S3、Google Cloud Storage）存储大规模数据。

数据管理：
	清洗和处理： 对原始数据进行清洗和处理，处理缺失值、重复项、异常值等。
	数据标准化： 统一数据格式和单位，以确保数据的一致性。
	元数据管理： 记录和管理数据的元信息，包括数据来源、结构和含义等。
	数据二次加工：对基础数据统计和计算得到二次数据，给上层调用，也可以理解为从其他地方获取数据文件，这里对数据变化改造一下符合自己代码格式

数据调用：
	API： 创建自己的API或使用现有的API，以便其他应用程序可以通过网络调用数据。
	查询语言： 使用SQL或类似的查询语言从数据库中检索数据。
	Web服务： 将数据提供为Web服务，可以通过HTTP或其他协议访问。



 
四个模块，需要隔离，不要相互嵌套，注意接口统一


这里案例就是微博爬取那个代码，看一下之后再重构自己代码，不要自己先写，学习案例后，灭有那么复杂，学习一下



理论上这个‘数据’模块只有‘数据调用’是个外界联系的，其他模块本地操作。不要在其他地方使用。就是说有个本地.py完成数据获取工作，本地保存，数据管理。

调用这是这个本地py调用一下，而且尽量不要给外部


